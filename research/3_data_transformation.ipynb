{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/pc/Desktop/GitHub repos/End-to-end-ML-project-with-MLflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\pc\\\\Desktop\\\\GitHub repos\\\\End-to-end-ML-project-with-MLflow'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlProject import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import polars as pl\n",
    "from mlProject.entity.config_entity import DataValidationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, ValidationConfig: DataValidationConfig, TransformationConfig: DataTransformationConfig):\n",
    "        self.ValidationConfig = ValidationConfig\n",
    "        self.TransformationConfig = TransformationConfig\n",
    "\n",
    "    def preprocess_data(self) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Preprocesses Velib bike sharing dataframe by:\n",
    "        1. Parsing datetime column into date, time and weekday components\n",
    "        2. Calculating total available bikes and free terminals\n",
    "        3. Extracting latitude and longitude from station_geo\n",
    "        \n",
    "        Args:\n",
    "            data (pl.DataFrame): Raw Velib dataframe\n",
    "            \n",
    "        Returns:\n",
    "            pl.DataFrame: Preprocessed Velib dataframe with additional columns\n",
    "        \"\"\"\n",
    "\n",
    "        df = pl.read_csv(self.ValidationConfig.unzip_data_dir)\n",
    "\n",
    "        # filter out non-operative stations\n",
    "        df = df.filter(pl.col(\"operative\") == True)\n",
    "\n",
    "        # parse datetime column and extract date and time\n",
    "        df = df.with_columns(\n",
    "            # Step 1: Parse the string into a Datetime object\n",
    "            # Polars' default parser usually handles ISO 8601 format (T separator, Z for UTC)\n",
    "            datetime = pl.col(\"datetime\").str.to_datetime()\n",
    "        ).with_columns(\n",
    "            # Step 2: Extract Date, Time and Weekday from the new Datetime object\n",
    "            date = pl.col(\"datetime\").dt.date(),\n",
    "            time = pl.col(\"datetime\").dt.time(),\n",
    "            weekday = pl.col(\"datetime\").dt.weekday()\n",
    "        )\n",
    "        # Extract hour from time column\n",
    "        df = df.with_columns(\n",
    "            hour = pl.col(\"time\").cast(str).str.slice(0, 2).cast(pl.Int32)\n",
    "        )\n",
    "\n",
    "        # create new columns\n",
    "        total_available = pl.col(\"available_mechanical\") + pl.col(\"available_electrical\")\n",
    "\n",
    "        df = df.with_columns(\n",
    "            total_available = total_available,\n",
    "            free_terminals = pl.col(\"capacity\") - total_available,\n",
    "            lat = pl.col(\"station_geo\").str.split(\",\").list.first().cast(float),\n",
    "            lon = pl.col(\"station_geo\").str.split(\",\").list.last().cast(float)\n",
    "        )\n",
    "\n",
    "        # Group by date, hour, and station, taking last value in each hour\n",
    "        df = df.group_by([\"date\", \"hour\", \"station_name\"]).agg([\n",
    "            pl.col(\"weekday\").last(),\n",
    "            pl.col(\"lat\").last(),\n",
    "            pl.col(\"lon\").last(),\n",
    "            pl.col(\"total_available\").last(),\n",
    "            pl.col(\"available_mechanical\").last(), \n",
    "            pl.col(\"available_electrical\").last(),\n",
    "            pl.col(\"free_terminals\").last()\n",
    "        ])\n",
    "        \n",
    "        # drop datetime column and reorder columns\n",
    "        df = df.select([\n",
    "            \"date\",\n",
    "            \"weekday\",\n",
    "            \"hour\",\n",
    "            \"station_name\",\n",
    "            \"total_available\",\n",
    "            \"available_mechanical\",\n",
    "            \"available_electrical\",\n",
    "            \"free_terminals\"\n",
    "        ]).sort([\"date\", \"hour\", \"station_name\"])\n",
    "\n",
    "        logger.info(f\"Preprocessed data. Shape: {df.shape}\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def create_station_mapping(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a station mapping (station_id <=> station_name) for the dataframe.\n",
    "        \"\"\"\n",
    "        station_mapping = (\n",
    "            df.sort(\"station_name\")\n",
    "            .select(\"station_name\")\n",
    "            .unique()\n",
    "            .with_row_index(\"station_id\")\n",
    "        )\n",
    "        return station_mapping\n",
    "        \n",
    "\n",
    "\n",
    "    def create_lagged_features(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates lagged features for the dataframe.\n",
    "        \"\"\"\n",
    "        station_mapping = self.create_station_mapping(df)\n",
    "\n",
    "        # Add station IDs to original dataframe by joining on station_name\n",
    "        new_df = df.join(\n",
    "            station_mapping,\n",
    "            on=\"station_name\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        new_df = new_df.with_columns(\n",
    "            pl.col(\"station_id\").cast(pl.String).cast(pl.Categorical)\n",
    "        )\n",
    "\n",
    "        # Replace station_name with station_id and reorder columns\n",
    "        new_df = new_df.select([\n",
    "            \"station_id\",\n",
    "            \"date\",\n",
    "            \"hour\",\n",
    "            \"total_available\",\n",
    "            \"available_mechanical\", \n",
    "            \"available_electrical\",\n",
    "            \"free_terminals\"\n",
    "        ])\n",
    "        \n",
    "        # Sort by station_id, date and hour to ensure proper lag calculation\n",
    "        new_df = new_df.sort([\"station_id\", \"date\", \"hour\"])\n",
    "\n",
    "        # Get unique station IDs\n",
    "        unique_stations = new_df.get_column(\"station_id\").unique().to_list()\n",
    "\n",
    "        # Create list to store DataFrames with lagged features for each station\n",
    "        station_lag_dfs = []\n",
    "\n",
    "        # For each station, create lagged features\n",
    "        for station_id in unique_stations:\n",
    "        # Filter data for current station\n",
    "            station_df = new_df.filter(pl.col(\"station_id\") == station_id)\n",
    "            \n",
    "            # Create lag columns for total_available\n",
    "            lag_columns = []\n",
    "            for i in range(1, 25):\n",
    "                lag_columns.append(\n",
    "                    pl.col(\"total_available\").shift(i).alias(f\"total_available_lag_{i}\")\n",
    "                )\n",
    "            \n",
    "            # Add lag columns to station DataFrame\n",
    "            station_with_lags = station_df.with_columns(lag_columns).select(\n",
    "                [\"station_id\", \"date\", \"hour\", \"total_available\"] + \n",
    "                [f\"total_available_lag_{i}\" for i in range(1, 25)]\n",
    "            )\n",
    "            \n",
    "            # Drop rows with any null values (first 24 hours)\n",
    "            station_with_lags = station_with_lags.drop_nulls()\n",
    "            \n",
    "            if len(station_with_lags) > 0:  # Only append if we have data after dropping nulls\n",
    "                station_lag_dfs.append(station_with_lags)\n",
    "        \n",
    "        # Concatenate all station_lag_dfs vertically to form the final df\n",
    "        lags_df = pl.concat(station_lag_dfs).sort([\"date\", \"hour\"])\n",
    "\n",
    "        logger.info(f\"Created lagged features dataframe. Shape: {lags_df.shape}\")\n",
    "        \n",
    "\n",
    "        return lags_df\n",
    "\n",
    "\n",
    "\n",
    "    def split_train_test(self, df):\n",
    "        # Define the split point (e.g., 95% for training)\n",
    "        split_ratio = 0.95\n",
    "        split_index = int(len(df) * split_ratio)\n",
    "\n",
    "        # split data temporally\n",
    "        train = df[:split_index]\n",
    "        test = df[split_index:]\n",
    "\n",
    "        # save to csv files annd store them in the given dir\n",
    "        train.write_csv(os.path.join(self.TransformationConfig.root_dir, \"train.csv\"))\n",
    "        test.write_csv(os.path.join(self.TransformationConfig.root_dir, \"test.csv\"))\n",
    "\n",
    "        logger.info(\"Data split into training and test sets\")\n",
    "        print(\"train.shape\", train.shape)\n",
    "        print(\"test.shape\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 20:19:26,755: INFO: common: yaml file: schema.yaml loaded successfully]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConfigBox({'COLUMNS': {'datetime': 'String,', 'capacity': 'Int64,', 'available_mechanical': 'Int64,', 'available_electrical': 'Int64,', 'station_name': 'String,', 'station_geo': 'String,', 'operative': 'Boolean'}, 'TARGET_COLUMN': {'name': 'total_available'}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.mlProject.utils.common import read_yaml\n",
    "\n",
    "schema_filepath = Path(\"schema.yaml\")\n",
    "read_yaml(schema_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 20:19:26,794: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-05-15 20:19:26,797: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-05-15 20:19:26,800: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-05-15 20:19:26,801: INFO: common: created directory at: artifacts]\n",
      "[2025-05-15 20:19:26,802: INFO: common: created directory at: artifacts/data_validation]\n",
      "[2025-05-15 20:19:26,804: INFO: common: created directory at: artifacts/data_transformation]\n"
     ]
    }
   ],
   "source": [
    "from mlProject.config.configuration import ConfigurationManager\n",
    "\n",
    "CM = ConfigurationManager()\n",
    "ValidationConfig = CM.get_data_validation_config()\n",
    "TransformConfig = CM.get_data_transformation_config()\n",
    "datatransformation = DataTransformation(ValidationConfig, TransformConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 20:19:34,804: INFO: 2274250724: Preprocessed data. Shape: (4205848, 8)]\n"
     ]
    }
   ],
   "source": [
    "processed_df = datatransformation.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 20:19:39,516: INFO: 2274250724: Created lagged features dataframe. Shape: (4172748, 28)]\n"
     ]
    }
   ],
   "source": [
    "lags_df = datatransformation.create_lagged_features(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-15 20:19:40,432: INFO: 2274250724: Data split into training and test sets]\n",
      "train.shape (3964110, 28)\n",
      "test.shape (208638, 28)\n"
     ]
    }
   ],
   "source": [
    "datatransformation.split_train_test(lags_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
