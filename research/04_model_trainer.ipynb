{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\pc\\\\Desktop\\\\GitHub repos\\\\End-to-end-ML-project-with-MLflows'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/pc/Desktop/GitHub repos/End-to-end-ML-project-with-MLflows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\pc\\\\Desktop\\\\GitHub repos\\\\End-to-end-ML-project-with-MLflows'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    model_name: str\n",
    "    objective: str\n",
    "    metric: str           \n",
    "    boosting_type: str\n",
    "    num_leaves: int\n",
    "    learning_rate: float\n",
    "    feature_fraction: float\n",
    "    n_estimators: int\n",
    "    target_column: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlProject.constants import *\n",
    "from mlProject.utils.common import read_yaml, create_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_dirs([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.LightGBM\n",
    "        target =  self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_dirs([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path = config.train_data_path,\n",
    "            test_data_path = config.test_data_path,\n",
    "            model_name = config.model_name,\n",
    "            objective = params.objective,\n",
    "            metric = params.metric ,           \n",
    "            boosting_type = params.boosting_type,\n",
    "            num_leaves = params.num_leaves,\n",
    "            learning_rate = params.learning_rate,\n",
    "            feature_fraction = params.feature_fraction,\n",
    "            n_estimators = params.n_estimators,\n",
    "            target_column = target.name\n",
    "            \n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04 23:00:39,255: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-05-04 23:00:39,259: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-05-04 23:00:39,261: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-05-04 23:00:39,263: INFO: common: created directory at: artifacts]\n",
      "[2025-05-04 23:00:39,265: INFO: common: created directory at: artifacts/model_trainer]\n"
     ]
    }
   ],
   "source": [
    "cm = ConfigurationManager()\n",
    "config = cm.get_model_trainer_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelTrainerConfig(root_dir='artifacts/model_trainer', train_data_path='artifacts/data_transformation/train.csv', test_data_path='artifacts/data_transformation/test.csv', model_name='model.joblib', objective='regression_l1', metric='mae', boosting_type='gbdt', num_leaves=31, learning_rate=0.05, feature_fraction=0.9, n_estimators=400, target_column='total_available')\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "from mlProject import logger\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        # read the train and test datasets using polars since it's faster\n",
    "        train_data = pl.read_csv(self.config.train_data_path)\n",
    "        test_data = pl.read_csv(self.config.test_data_path)\n",
    "\n",
    "        # convert to pandas dataframe since the model handles them better\n",
    "        train_data = train_data.to_pandas() \n",
    "        test_data = test_data.to_pandas()\n",
    "        print(train_data['date'].dtype)\n",
    "\n",
    "        X_train = train_data.drop(['date', self.config.target_column], axis=1)\n",
    "        X_test = test_data.drop(['date', self.config.target_column], axis=1)\n",
    "        y_train = train_data[[self.config.target_column]]\n",
    "        y_test = test_data[[self.config.target_column]]\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "        # Create LightGBM Dataset objects\n",
    "        # It automatically detects 'category' dtype columns\n",
    "        lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False) # Keep raw data if needed later\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train, free_raw_data=False)\n",
    "\n",
    "        # Define model parameters\n",
    "        params = {\n",
    "            'objective': self.config.objective,  \n",
    "            'metric': self.config.metric,              \n",
    "            'boosting_type': self.config.boosting_type,\n",
    "            'num_leaves': self.config.num_leaves,\n",
    "            'learning_rate': self.config.learning_rate,\n",
    "            'feature_fraction': self.config.feature_fraction,\n",
    "            'random_state': 42,\n",
    "            'verbose': -1,  # avoids surpressing training process messages\n",
    "            'n_estimators': self.config.n_estimators,     \n",
    "            'n_jobs': -1   # Use all available CPU cores\n",
    "        }\n",
    "\n",
    "        # Train the model\n",
    "        lgbm = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=1000, # Max rounds\n",
    "                        valid_sets=[lgb_train, lgb_eval],\n",
    "                        valid_names=['train', 'eval'],\n",
    "                        callbacks=[lgb.early_stopping(10), lgb.log_evaluation(period=50)])\n",
    "        \n",
    "        return lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04 23:29:11,807: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-05-04 23:29:11,811: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-05-04 23:29:11,815: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-05-04 23:29:11,817: INFO: common: created directory at: artifacts]\n",
      "[2025-05-04 23:29:11,819: INFO: common: created directory at: artifacts/model_trainer]\n",
      "object\n",
      "X_train shape: (4043329, 26), y_train shape: (4043329, 1)\n",
      "X_test shape: (212807, 26), y_test shape: (212807, 1)\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttrain's l1: 1.84848\teval's l1: 1.68921\n",
      "[100]\ttrain's l1: 1.47289\teval's l1: 1.48833\n",
      "[150]\ttrain's l1: 1.40354\teval's l1: 1.45945\n",
      "[200]\ttrain's l1: 1.38573\teval's l1: 1.45016\n",
      "[250]\ttrain's l1: 1.37613\teval's l1: 1.44441\n",
      "[300]\ttrain's l1: 1.37174\teval's l1: 1.4417\n",
      "[350]\ttrain's l1: 1.36661\teval's l1: 1.43933\n",
      "[400]\ttrain's l1: 1.36231\teval's l1: 1.43687\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[400]\ttrain's l1: 1.36231\teval's l1: 1.43687\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer_config.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
